---
# Environment data file

# IMPORTANT: Whatever data is here will be appended to the overall `build.yaml` that will be located `public` or
# `private` based on the value passed to the intial ./CEPH_UP script

# Things specific to the environment of which you're building. The default is 'vagrant' for a 'local' build.

environment: vagrant

# Domain will be your actual domain in your production cluster
domain: example.com

# Default data center name is `local`
data_center: &data_center local

# Can only be `public` or `private`.
location: public

# This is used for backups and relates to backup in common.yaml. It's here because the remote hosts are environment specific.
rsync:
    hosts:
        # Should be the remote host where to rsync the backups to.
        -   ceph-vm3

# NICs - Each hardware vendor *may* have different device names. Define them here for the given environment
# Supermicro
## Non-Storage nodes - Assumes 2 devices (could be any size 10Gb, 20Gb, 40Gb) in this scenario
# Non-Storage nodes (clients) do not need a second NIC/Port since only the "public" interface is used
# If you have two then you can bond them if desired
# device1_1U: &device1_1U enp4s0f0
# device2_1U: &device2_1U enp4s0f1d1
## Storage nodes - Assumes 2 devices (could be any size 10Gb, 20Gb, 40Gb) in this scenario
# device1_2U: &device1_2U enp130s0f0
# device2_2U: &device2_2U enp130s0f1d1

## Make sure to check the NIC device names of your hardware and update them here.

## NB: Important - The following device variables are defined here in the environment.yaml file ONLY.
## Other data files can reference `*device1_1U` etc since the final file 'build.yaml' will have these defined in it.
#
# Vagrant
device1_1U: &device1_1U enp0s8
device2_1U: &device2_1U enp0s9
device1_2U: &device1_2U enp0s8
device2_2U: &device2_2U enp0s9
#
## End of device variable naming

# Set the proxies for the given environment if there are any.
proxy:
    http: ""
    https: ""

# These can be any data files you want. A normal example is putting your rack data into a separate yaml for each rack
# and then add them here to be included in the final build of `build.yaml` which everything will eventually derive from.
data_files:
    - "ceph.yaml"
    - "openstack.yaml"
    - "adc.yaml"
    - "centos.yaml"
    - "pxe_boot.yaml"
    - "inventory.yaml"

# Environment specific files that need to run through the template engine
# *.env files contain host names for given roles which comes from inventory.yaml.
environment_files:
    -   "bootstrap/vagrant/environment_config.yaml"
    -   "bootstrap/vagrant/servers_config.yaml"
    -   "bootstrap/vagrant/CEPH_UP_VAGRANT"
    -   "environments/ceph_chef_adc_hosts.env"
    -   "environments/ceph_chef_admin_hosts.env"
    -   "environments/ceph_chef_bootstrap.env"
    -   "environments/ceph_chef_hosts.env"
    -   "environments/ceph_chef_mds_hosts.env"
    -   "environments/ceph_chef_mgr_hosts.env"
    -   "environments/ceph_chef_mon_hosts.env"
    -   "environments/ceph_chef_osd_hosts.env"
    -   "environments/ceph_chef_proxy.env"
    -   "environments/ceph_chef_rgw_hosts.env"

# Place all files that fall under the `cookbooks` directory normally such as recipes, recipe templates, etc
cookbooks_files:
    -   "cookbooks/cepheus/recipes/cobbler-install.rb"
    -   "cookbooks/cepheus/recipes/cobbler-sync.rb"
    -   "cookbooks/cepheus/templates/default/cobbler.dhcp.template.erb"
    -   "cookbooks/cepheus/templates/default/zabbix-userparameter-radosgw.conf.erb"
    -   "cookbooks/cepheus/metadata.rb"

# By default 'ceph-chef' gets downloaded in the 'bootstrap_prereqs.sh' script. However, there are times when you
# may want a different version. This section will allow for adding environment specific 'staging' like repo endpoints
# that will override existing versions of specific repos...
# NB: Staging Layers
layers:
    staging:
        access: internet
        description: "Staging Layers - Code"
        process:
            -   name: "Cepheus Public Repo Layer"
                output: "$WORK_DIR/cepheus-public-staging"
                comment: "Public Cepheus repo"
                repo: https://github.com/cepheus-io/cepheus.git
                branch: master
                command: ""
            -   name: "Cepheus Private Repo Layer"
                output: "$WORK_DIR/cepheus-private-staging"
                comment: "Private Cepheus repo. Here to show that there could be a private version with your company specific items"
                repo: https://github.com/cepheus-io/cepheus-private
                branch: master
                command: ""
            -   name: "Ceph-Chef Public Repo Layer"
                output: "$WORK_DIR/ceph-chef-public-staging"
                comment: "Public Ceph-Chef Cookbook repo"
                repo: https://github.com/ceph/ceph-chef.git
                branch: master
                command: ""
            -   name: "Ceph-Chef Private Repo Layer"
                output: "$WORK_DIR/ceph-chef-private-staging"
                comment: "Private Ceph-Chef. Here to show that there could be a private version with your company specific items"
                repo: https://github.com/cepheus-io/ceph-chef
                branch: master
                command: ""
            -   name: "Cepheus Data Repo Layer"
                output: "$WORK_DIR/cepheus-data-staging"
                comment: "Private data. This one is public but should be private for production envrionments"
                repo: https://github.com/cepheus-io/cepheus-data
                branch: master
                command: ""
            -   name: "Layering Code"
                output: ""
                comment: "Layering data. Cepheus files are basically dependencies. This can be a git repo, object-storage or whatever. Just make sure the data is fed into 'output'"
                repo: ""
                branch: ""
                # NB: The below $ variables are present in the script this will go in so keep that in mind. Need to create global vars later.
                # NB: This example is for vagrant so the commands below would be different for production such as calling bootstrap_prereqs.sh
                # may not be called in a production environment if it had no access to the outside world. Instead it may pull from a
                # private git repo or something like artifactory. Below it's redundant...
                # NB: When using '|' even embedded '#' are not treated as comments in yaml but pass through as desired.
                # Empty lines also pass through.
                command: |
                    echo
                    echo_yellow "====> Layering data..."

                    cp -rp $WORK_DIR/cepheus-private-staging/* $WORK_DIR/cepheus-public-staging/
                    cp -rp $WORK_DIR/ceph-chef-private-staging/* $WORK_DIR/ceph-chef-public-staging/
                    # NB: Data is required before generating files from templates
                    cp -rp $WORK_DIR/cepheus-data-staging/* $WORK_DIR/cepheus-public-staging/

                    # Need to generate 'build.yaml' from all of the other data files before calling bootstrap_prereqs.sh...
                    # CEPH_UP being called to generate files...
                    cd $WORK_DIR/cepheus-public-staging && ./CEPH_UP -x -z $WORK_DIR/cepheus-public-staging
                    echo
                    echo_yellow "====> Generating bootstrap_prereqs.sh..."
                    source $WORK_DIR/cepheus-public-staging/bootstrap/common/bootstrap_prereqs.sh $WORK_DIR/cepheus-files-staging $WORK_DIR/cepheus-public-staging
                    ls $WORK_DIR/cepheus-files-staging
            -   name: "Layering Cookbooks"
                output: ""
                comment: "Layer cookbooks specifically for this environment or generically..."
                repo: ""
                branch: ""
                command: |
                    echo
                    echo_yellow "====> Layering cookbooks..."

                    cp $WORK_DIR/cepheus-files-staging/cookbooks/*.tar.gz $WORK_DIR/cepheus-public-staging/cookbooks

                    cd $WORK_DIR/cepheus-public-staging/cookbooks && ls -1 *.tar.gz | xargs -I% tar xvzf %
                    cd $WORK_DIR/cepheus-public-staging/cookbooks && rm -f *.tar.gz
                    sudo chown -R $OWNER:$GROUP $WORK_DIR/cepheus-public-staging

                    # Override ceph-chef - Useful if debugging or custom version
                    cp -rp $WORK_DIR/ceph-chef-public-staging/* $WORK_DIR/cepheus-public-staging/cookbooks/ceph-chef/

# Info to setup development environment as well
development:
    enable: false
    user: vagrant

# Root/sudo related items
root:
    # Used in bootstrap kickstart file only for now which is used only for PXE booting
    pwd: $6$Salt$3xxLPT099nzTbWkOS3CPNcar/zSLQ8BEgZdJk/AOkOb4V80sPepbraWcvrJvEEu6PswpKUw1WodWeiqRo1bw2/

# The users listed here will be created on all nodes in the cluster if they do not already exist
# To create encrypted pwd: python -c 'import crypt; print(crypt.crypt("password", "$6$Salt"))'
users:
    -   name: operations
        group: wheel
        comment: "Operations user"
        passwd: $6$Salt$3xxLPT099nzTbWkOS3CPNcar/zSLQ8BEgZdJk/AOkOb4V80sPepbraWcvrJvEEu6PswpKUw1WodWeiqRo1bw2/
        passwd_type: "--iscrypted"
        key:
            # 'enable' does not apply to initial keys for vagrant but only adding keys once on the node.
            enable: false
            # NB: If 'public_key' starts with 'ssh-rsa' then it's assumed the key is content and not a file. EXCEPT for PXE_BOOT - must be content
            # NB: 'cepheus' keys were created with 'ssh-keygen -b 2048 -t rsa -f $HOME/.ssh/cepheus -C "Cepheus example"'
            public: |
                ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCTHWJoLm/RpQZf5MmR+hgZ229GqppcxZzDpTVvcw1gYe8mLCNEH2jQZbgpLLJxgycdPLhc7Dcf8tvL3fK+Ron7Ph98WRmmJvz81iCKqEGkgS8r0GBQ2qexn1pEcAO6zLT9Yr2ZET35coPsIQYWGA1ER+7/S1irWhX/A4a6nh2dj3JNS3ewlxTnJaSdUUyhA1nE7lCrYQ53SELgX8gVuE54yN6pXnBL0u+GITEfn1oSvQ10gA+HEV6nAcmfE6T0wji0nTmXHmTSNpzzSjEyvF/blIc9fsXf6YyhpzBmZ3/72+OgLAIEFKAnRydH8YO5or3S85hxfNe8cO0gZDVU9WdP Cepheus example
            private: cepheus
            # 'key_path' represents where the keys are located on *your* machine
            # Put or create keys in a directory that is *NOT* user specific but generic so it works with anyones build such as '$HOME/.ssh' or
            # something like '/keys'
            path: $HOME/.ssh
            # NB: Key management should be done elsewhere. Cepheus will check for existing file and take it. If one is not found then
            # it will create one using the passphrase here. You can use this to get you started but it is strongly recommended that
            # you change your ssh key but use the same name later if you don't already have one!
            passphrase: "cepheus"
        ceph_group: true
    -   name: vagrant
        group: wheel
        comment: "Vagrant user"
        passwd: $6$Salt$6AyUczFy6SgV8A2wKAKfA9drpzrUsTGPJ3QjcWBbgS97BxBO.C7ZcBFALRiRkKfi9x8MK2SHet38BCQWS9LsR/
        passwd_type: "--iscrypted"
        key:
            # 'enable' does not apply to initial keys for vagrant but only adding keys once on the node.
            enable: false
            # NB: If 'public_key' starts with 'ssh-rsa' then it's assumed the key is content and not a file. EXCEPT for PXE_BOOT - must be content
            # NB: 'cepheus' keys were created with 'ssh-keygen -b 2048 -t rsa -f $HOME/.ssh/cepheus -C "Cepheus example"'
            public: |
                ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCTHWJoLm/RpQZf5MmR+hgZ229GqppcxZzDpTVvcw1gYe8mLCNEH2jQZbgpLLJxgycdPLhc7Dcf8tvL3fK+Ron7Ph98WRmmJvz81iCKqEGkgS8r0GBQ2qexn1pEcAO6zLT9Yr2ZET35coPsIQYWGA1ER+7/S1irWhX/A4a6nh2dj3JNS3ewlxTnJaSdUUyhA1nE7lCrYQ53SELgX8gVuE54yN6pXnBL0u+GITEfn1oSvQ10gA+HEV6nAcmfE6T0wji0nTmXHmTSNpzzSjEyvF/blIc9fsXf6YyhpzBmZ3/72+OgLAIEFKAnRydH8YO5or3S85hxfNe8cO0gZDVU9WdP Cepheus example
            private: cepheus
            # 'key_path' represents where the keys are located on *your* machine
            # Put or create keys in a directory that is *NOT* user specific but generic so it works with anyones build such as '$HOME/.ssh' or
            # something like '/keys'
            path: $HOME/.ssh
            # NB: Key management should be done elsewhere. Cepheus will check for existing file and take it. If one is not found then
            # it will create one using the passphrase here. You can use this to get you started but it is strongly recommended that
            # you change your ssh key but use the same name later if you don't already have one!
            passphrase: "cepheus"
        sudo: true
        shell: /bin/bash
        # For Jewel and later the `ceph_group` tells the recipes to add this user to the 'ceph' group so 'sudo' is not
        # required for ceph commands
        ceph_group: true

# This user will be the primary user of the given envrionment which will be `operations` in production and `vagrant` for local
# This is can be a mirror of one of the users above. It must be present.
primary_user:
    name: vagrant
    group: wheel
    comment: "Vagrant user"
    passwd: $6$Salt$6AyUczFy6SgV8A2wKAKfA9drpzrUsTGPJ3QjcWBbgS97BxBO.C7ZcBFALRiRkKfi9x8MK2SHet38BCQWS9LsR/
    passwd_type: "--iscrypted"
    key:
        # 'enable' does not apply to initial keys for vagrant but only adding keys once on the node.
        enable: false
        # NB: If 'public_key' starts with 'ssh-rsa' then it's assumed the key is content and not a file. EXCEPT for PXE_BOOT - must be content
        # NB: 'cepheus' keys were created with 'ssh-keygen -b 2048 -t rsa -f $HOME/.ssh/cepheus -C "Cepheus example"'
        public: |
            ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCTHWJoLm/RpQZf5MmR+hgZ229GqppcxZzDpTVvcw1gYe8mLCNEH2jQZbgpLLJxgycdPLhc7Dcf8tvL3fK+Ron7Ph98WRmmJvz81iCKqEGkgS8r0GBQ2qexn1pEcAO6zLT9Yr2ZET35coPsIQYWGA1ER+7/S1irWhX/A4a6nh2dj3JNS3ewlxTnJaSdUUyhA1nE7lCrYQ53SELgX8gVuE54yN6pXnBL0u+GITEfn1oSvQ10gA+HEV6nAcmfE6T0wji0nTmXHmTSNpzzSjEyvF/blIc9fsXf6YyhpzBmZ3/72+OgLAIEFKAnRydH8YO5or3S85hxfNe8cO0gZDVU9WdP Cepheus example
        private: cepheus
        # 'key_path' represents where the keys are located on *your* machine
        # Put or create keys in a directory that is *NOT* user specific but generic so it works with anyones build such as '$HOME/.ssh' or
        # something like '/keys'
        path: $HOME/.ssh
        # NB: Key management should be done elsewhere. Cepheus will check for existing file and take it. If one is not found then
        # it will create one using the passphrase here. You can use this to get you started but it is strongly recommended that
        # you change your ssh key but use the same name later if you don't already have one!
        passphrase: "cepheus"
    sudo: true
    # For Jewel and later the `ceph_group` tells the recipes to add this user to the 'ceph' group so 'sudo' is not
    # required for ceph commands
    ceph_group: true

# Data specific for the given bootstrap node. This is environment specific data.
bootstrap:
    # Bootstrap IP address
    ip: 10.0.100.20
    # Host name
    name: ceph-bootstrap
    # Used by scripts in the cookboks. The user MUST be correct. The default user is `operations`
    env: /home/vagrant/cepheus/environments
    # ks - Kickstart file in RHEL/CentOS
    ks: cepheus_bootstrap_rhel.ks
    # Roles are important. Roles define what gets installed and what purpose the node plays in your cluster.
    # You can combine any number of roles the node is suppose to have as long as a given role does not conflict with another.
    roles:
        - bootstrap
    interfaces:
      - device: *device1_1U
        ip: 10.0.100.20
        mtu: 1500
        netmask: 255.255.255.0
        gateway: ""
        mac: ""
      - device: *device2_1U
        ip: 10.121.2.2
        mtu: 1500
        netmask: 255.255.255.0
        gateway: ""
        mac: ""

chef:
    owner: vagrant
    org: cepheus
    name: Cepheus - LambdaStack.io
    validator: cepheus
    admin:
        user: admin
        email: admin@localhost.com
        pem: admin.pem
    # May want the group part of the default group of the owner above.
    group: vagrant
    server: http://10.0.100.20:4000
    validation_client_name: cepheus

monitoring:
  collectd:
    id: graphite
    ip: 127.0.0.1
    port: 2013
    prefix: "collectd."
  zabbix:
    server: 10.0.100.6
    ceph:
      blocked_op: 0
      slow_osd: 0

cron:
    radosgw:
        # The radosgw-stats.sh gets created based on the environment/data center and runs every 30 minutes
        # to push stats to graphite to be graphed elsewhere
        stats:
            minute: "/30"
            command: "/usr/local/bin/radosgw-stats.sh"

    # Log processing to push logs into Cepheus
    logs:
        enable: false
        radosgw:
            uid: cepheus_log_admin
            name: "Cepheus Log Admin"
            access_key: "whatever_your_access_key"
            secret_key: "whatever_your_secret_key"
            endpoint: "whatever_your_endpoint"
            port: 80
            bucket: cepheus_logs
        patterns:
            -   directory: "/var/log/ceph"
                pattern: "*.log-*.gz"
            -   directory: "/var/log/radosgw"
                pattern: "ceph.client.radosgw.*.log-*.gz"
            # Broken out here to show that you can if you wanted to push the data to a different bucket
            -   directory: "/var/log/radosgw"
                pattern: "civetweb.access.*.log-*.gz"
                # Bucket here is to show that down the road we can add this feature maybe if we want to put the access logs in a different bucket
                #bucket: cepheus_logs_access

# Supermicro node NICs look like:
#   NOTE: 1U nodes have the following interface names:
#   enp4s0f0 - public and enp4s0f1d1 - cluster
#   NOTE: 2U nodes have the following interface names:
#   enp130s0f0 - public and enp130s0f1d1 - cluster
#   2U Interfaces
network:
  public:
    interface: *device1_2U
    cidr:
        # If each rack was on a different subnet then list them for each rack
        - 10.0.100.0/24
    netmask: 255.255.255.0
    # Change the MTU on production systems to 9000
    mtu: 1500
  cluster:
    interface: *device2_2U
    gateway_enable: True # false for production
    cidr:
        - 10.121.2.0/24
    route:
        cidr: ""
    netmask: 255.255.255.0
    # Change the MTU on production systems to 9000
    mtu: 1500

# Example of bond if needed. Must enable it.
# REMOVE LATER - Moved to Inventory.yaml
bond:
  name: bond0
  enable: false
  mtu: 1500
  # NB: Hardware level options
  # options: "mode=4 miimon=100 xmit_hash_policy=layer3+4"
  # NB: VBox level options
  options: "mode=1 miimon=100 fail_over_mac=1"

  interfaces:
    - *device1_1U
    - *device1_2U

# Put your DNS servers here
nameservers:
  - 8.8.8.8
  - 8.8.4.4

# Put your NTP servers here (host name or IP)
ntp:
    - 0.pool.ntp.org
    - 1.pool.ntp.org
    - 2.pool.ntp.org
    - 3.pool.ntp.org

# Determine if the 'ceph' repo should be created. This is usually 'true' except for 'rhel' under 'subscription'
ceph_repo_create: true
