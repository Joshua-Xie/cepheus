---
# Environment data file

# IMPORTANT: Whatever data is here will be appended to the overall `build.yaml` that will be located `public` or
# `private` based on the value passed to the intial ./CEPH_UP script

# Things specific to the environment of which you're building. The default is 'vagrant' for a 'local' build.

environment: vagrant

# Domain will be your actual domain in your production cluster
domain: example.com

# Default data center name is `local`
data_center: &data_center local

# Can only be `public` or `private`.
location: public

# NICs - Each hardware vendor *may* have different device names. Define them here for the given environment
# Supermicro
## Non-Storage nodes - Assumes 2 devices (could be any size 10Gb, 20Gb, 40Gb) in this scenario
# Non-Storage nodes (clients) do not need a second NIC/Port since only the "public" interface is used
# If you have two then you can bond them if desired
# device1_1U: &device1_1U enp4s0f0
# device2_1U: &device2_1U enp4s0f1d1
## Storage nodes - Assumes 2 devices (could be any size 10Gb, 20Gb, 40Gb) in this scenario
# device1_2U: &device1_2U enp130s0f0
# device2_2U: &device2_2U enp130s0f1d1

## Make sure to check the NIC device names of your hardware and update them here.

## NB: Important - The following device variables are defined here in the environment.yaml file ONLY.
## Other data files can reference `*device1_1U` etc since the final file 'build.yaml' will have these defined in it.
#
# Vagrant
device1_1U: &device1_1U enp0s8
device2_1U: &device2_1U enp0s9
device1_2U: &device1_2U enp0s8
device2_2U: &device2_2U enp0s9
#
## End of device variable naming

# These can be any data files you want. A normal example is putting your rack data into a separate yaml for each rack
# and then add them here to be included in the final build of `build.yaml` which everything will eventually derive from.
data_files:
    - "centos.yaml"
    - "pxe_boot.yaml"
    - "rack_01.yaml"
    - "rack_02.yaml"
    - "rack_03.yaml"

# Environment specific files that need to run through the template engine
environment_files:
    - "bootstrap/vms/environment_config_yaml"

# Root/sudo related items
root:
    # Used in bootstrap kickstart file only for now which is used only for PXE booting
    pwd: $6$Salt$3xxLPT099nzTbWkOS3CPNcar/zSLQ8BEgZdJk/AOkOb4V80sPepbraWcvrJvEEu6PswpKUw1WodWeiqRo1bw2/

# The users listed here will be created on all nodes in the cluster if they do not already exist
# To create encrypted pwd: python -c 'import crypt; print(crypt.crypt("password", "$6$Salt"))'
users:
    - name: operations
      group: wheel
      comment: "Operations user"
      passwd: $6$Salt$3xxLPT099nzTbWkOS3CPNcar/zSLQ8BEgZdJk/AOkOb4V80sPepbraWcvrJvEEu6PswpKUw1WodWeiqRo1bw2/
      passwd_type: "--iscrypted"
      key: "ceph_bootstrap.pub"
      private_key: operations
      sudo: true
      # For Jewel and later the `ceph_group` tells the recipes to add this user to the 'ceph' group so 'sudo' is not
      # required for ceph commands
      ceph_group: true
    - name: vagrant
      group: wheel
      comment: "Vagrant user"
      passwd: $6$Salt$6AyUczFy6SgV8A2wKAKfA9drpzrUsTGPJ3QjcWBbgS97BxBO.C7ZcBFALRiRkKfi9x8MK2SHet38BCQWS9LsR/
      passwd_type: "--iscrypted"
      key: "ceph_bootstrap.pub"
      private_key: operations
      sudo: true
      # For Jewel and later the `ceph_group` tells the recipes to add this user to the 'ceph' group so 'sudo' is not
      # required for ceph commands
      ceph_group: true

# This user will be the primary user of the given envrionment which will be `operations` in production and `vagrant` for local
# This is can be a mirror of one of the users above. It must be present.
primary_user:
    name: vagrant
    group: wheel
    comment: "Vagrant user"
    passwd: $6$Salt$6AyUczFy6SgV8A2wKAKfA9drpzrUsTGPJ3QjcWBbgS97BxBO.C7ZcBFALRiRkKfi9x8MK2SHet38BCQWS9LsR/
    passwd_type: "--iscrypted"
    key: "ceph_bootstrap.pub"
    private_key: operations
    sudo: true
    # For Jewel and later the `ceph_group` tells the recipes to add this user to the 'ceph' group so 'sudo' is not
    # required for ceph commands
    ceph_group: true

# Data specific for the given bootstrap node. This is environment specific data.
bootstrap:
    # Bootstrap IP address
    ip: 10.0.100.20
    # Host name
    name: ceph-bootstrap
    # Used by scripts in the cookboks. The user MUST be correct. The default user is `operations`
    env: /home/operations/cepheus/environments
    # ks - Kickstart file in RHEL/CentOS
    ks: cepheus_bootstrap_rhel.ks
    # Roles are important. Roles define what gets installed and what purpose the node plays in your cluster.
    # You can combine any number of roles the node is suppose to have as long as a given role does not conflict with another.
    roles:
        - bootstrap
    interfaces:
      - device: *device1_1U
        ip: 10.0.100.20
        mtu: 1500
        netmask: 255.255.255.0
        gateway:
        mac:
      - device: *device2_1U
        ip: 10.121.2.2
        mtu: 1500
        netmask: 255.255.255.0
        gateway:
        mac:

# Hardware vendor specific
ipmi:
  user:
  passwd:

chef:
  owner: vagrant
  group: vagrant
  server: http://10.0.100.20:4000

monitoring:
  collectd:
    id: graphite
    ip: 127.0.0.1
    port: 2013
    prefix: "collectd."
  zabbix:
    server: 10.0.100.6
    ceph:
      blocked_op: 0
      slow_osd: 0

cron:
  # Log processing to push logs into Cepheus
  logs:
    enable: false
    ceph:
      radosgw:
        uid: cepheus_log_admin
        name: "Cepheus Log Admin"
        access_key: "whatever_your_access_key"
        secret_key: "whatever_your_secret_key"
        endpoint: "whatever_your_endpoint"
        port: 80
        bucket: cepheus_logs
    patterns:
      - directory: "/var/log/ceph"
        pattern: "*.log-*.gz"
      - directory: "/var/log/radosgw"
        pattern: "ceph.client.radosgw.*.log-*.gz"
      # Broken out here to show that you can if you wanted to push the data to a different bucket
      - directory: "/var/log/radosgw"
        pattern: "civetweb.access.*.log-*.gz"
        # Bucket here is to show that down the road we can add this feature maybe if we want to put the access logs in a different bucket
        #bucket: cepheus_logs_access

# Supermicro node NICs look like:
#   NOTE: 1U nodes have the following interface names:
#   enp4s0f0 - public and enp4s0f1d1 - cluster
#   NOTE: 2U nodes have the following interface names:
#   enp130s0f0 - public and enp130s0f1d1 - cluster
#   2U Interfaces
network:
  public:
    interface: *device1_2U
    cidr:
        # If each rack was on a different subnet then list them for each rack
        - 10.0.100.0/24
    netmask: 255.255.255.0
    # Change the MTU on production systems to 9000
    mtu: 1500
  cluster:
    interface: *device2_2U
    gateway_enable: true # false for production
    cidr:
        - 110.121.2.0/24
    route:
        cidr: 10.121.2.0/24
    netmask: 255.255.255.0
    # Change the MTU on production systems to 9000
    mtu: 1500

# ADC - Application Delivery Controller (aka Load Balancers)
adc:
    enable: false
    interface: *device1_1U
    haproxy:
        enable: false
        port: 1936
        user: haproxy
        passwd: haproxy
        balance: roundrobin
        max_connections: 6000
    # Keepalived nodes should have at least two but maybe one per rack
    keepalived:
        enable: false
        passwd: keepalived
        servers:
            - name: ceph-vm3
              weight: 5
              interface: *device1_1U
              priority: 90
            - name: ceph-vm2
              weight: 5
              interface: *device1_1U
              priority: 100
            - name: ceph-vm1
              weight: 5
              interface: *device1_1U
              priority: 110
    # BGP uses the keepalived servers - Make sure the peer names DO NOT have spaces.
    bgp:
        enable: false
        asn: 65032
        interface: *device1_1U
        roles:
            - name: ceph-vm1
              role: primary
            - name: ceph-vm2
              role: secondary
        peers:
            - name: Spine1
              label: bgp_peer1
              ip: 10.121.16.8
            - name: Spine2
              label: bgp_peer2
              ip: 10.121.16.9
    vip:
        prefix: 10.121.0.16
        netmask: 255.255.255.240
        cidr: 28
        vips:
            # backend_port represents the port of the backend instance which overrides the default backend/server/port below. ONLY applies to Federated.
            # IMPORTANT: NON RGW VIPS MUST be first for template to work without re-working it!
            # WWW is the Web UI for the cluster
            - name: www
              ip: 10.121.16.17
              interface: *device1_1U
              type: web
              backend_label: web-ui-backend
              ssl: false
              # NOTE: Change this wildcard cert later
              cert: wildcard.s3.www.example.com.pem
              # Since using a wildcard the ssl_files can be just one file. However, the recipe will build from the ssl_files if more are present
              ssl_files:
                  - wildcard.s3.www.example.com.pem
              url: www.example.com
              backend_port: 8088
              region: *data_center
            # API is the RESTful api for the cluster
            - name: api
              ip: 10.121.16.18
              interface: *device1_1U
              type: api
              backend_label: restful-api-backend
              ssl: false
              # NOTE: Change this wildcard cert later
              cert: wildcard.s3.api.example.com.pem
              # Since using a wildcard the ssl_files can be just one file. However, the recipe will build from the ssl_files if more are present
              ssl_files:
                  - wildcard.s3.api.example.com.pem
              url: api.example.com
              backend_port: 8089
              region: *data_center
            - name: dev
              ip: 10.121.16.19
              interface: *device1_1U
              type: rgw
              backend_label: radosgw-http-backend
              ssl: false
              cert: wildcard.s3.vagrant.dev.example.com.pem
              # Since using a wildcard the ssl_files can be just one file. However, the recipe will build from the ssl_files if more are present
              ssl_files:
                  - wildcard.s3.vagrant.dev.example.com.pem
              url: s3.vagrant.dev.example.com
              backend_port: 8080
              # NOTE: region and zone are here as well as in the ceph-federated section for easier ceph.conf rgw manipulation. The other is used for something else but the data is the same.
              region: *data_center
              rgw_thread_pool: 100
              handles: 5
              threads: 100
            - name: prod
              ip: 10.121.16.20
              interface: *device1_1U
              type: rgw
              backend_label: radosgw-http-backend
              ssl: false
              cert: wildcard.s3.vagrant.prod.example.com.pem
              # Since using a wildcard the ssl_files can be just one file. However, the recipe will build from the ssl_files if more are present
              ssl_files:
                  - wildcard.s3.vagrant.prod.example.com.pem
              url: s3.vagrant.prod.example.com
              backend_port: 8081
              region: *data_center
              rgw_thread_pool: 100
              handles: 5
              threads: 100
    backend:
        # port - value below is the default port value for the backend. The VIPs backend_port will override this variable in the template for Federated (ONLY).
        # type - has a special meaning for rgw and federated use
        servers:
            - name: ceph-vm1
              type: rgw
              weight: 6
              port: 8080
              options: "check inter 2s rise 2 fall 3"
            - name: ceph-vm2
              type: rgw
              weight: 6
              port: 8080
              options: "check inter 2s rise 2 fall 3"
            - name: ceph-vm3
              type: rgw
              weight: 6
              port: 8080
              options: "check inter 2s rise 2 fall 3"

# Example of bond if needed. Must enable it.
bond:
  name: bond0
  enable: false
  mtu: 1500
  options: "mode=4 miimon=100 xmit_hash_policy=layer3+4"
  interfaces:
    - *device1_1U
    - *device1_2U

# Put your DNS servers here
nameservers:
  - 8.8.8.8
  - 8.8.4.4

# Put your NTP servers here (host name or IP)
ntp:
    - 0.pool.ntp.org
    - 1.pool.ntp.org
    - 2.pool.ntp.org
    - 3.pool.ntp.org

ceph: # Packages will come from enterprise repo manager OR ceph.com if access to outside world is allowed
    cluster: ceph
    name: luminous
    version: 10.2.6

    tcmalloc: # This becomes the default after Jewel
        enable: true

    ec:
        enable: true
        profile: cepheus
        plugin: jerasure
        directory: "/usr/lib64/ceph/erasure-code"
        ruleset_root: hdd
        ruleset_failure_domain: host
        technique: reed_sol_van
        # These numbers are based on the number of OSD nodes and racks. Adjust to fit the INITIAL cluster - not after the fact
        k: 2
        m: 1

    pools:
        crush_rule: 1

    pgs:
        num: 64
        calc:
            total_osds: 612
            target_pgs_per_osd: 200
            min_pgs_per_pool: 64
            replicated_size: 3
            erasure_size: 11
    repo:
        # This will tell ceph-chef cookbook not to create a yum repo because we will use RHEL Sat Server
        create: false

    restapi: # TODO: Change this later to support the VIP API
        url: api.example.com
        ip: 10.121.1.18
        port: 5080

    mgr:
        enable: false

    radosgw:
        bucket_shards: 5
        rados_handles: 1
        civetweb_threads: 100
        settings:
            pg_num: 64
            pgp_num: 64
            size: 3
            crush_ruleset: 1
            chooseleaf: host
            chooseleaf_type: 1
            type: erasure
            # OSD nodes per rack - helps determine crushmap feature for erasure coding
            nodes_per_rack: 17
        # This may need to move to a DB later...
        default_url: s3.example.com
        debug:
            logs:
                enable: true
                level: 2
        logs:
            ops:
                enable: true
            usage:
                enable: true
        # Quotas are in MB
        users:
            - uid: radosgw
              name: Admin
              admin_caps: "users=*;buckets=*;metadata=*;usage=*;zone=*"
              # Keys are not required and NOT there for real users.
              access_key: ""
              secret_key: ""
              email: ""
              max_buckets: 0
              # status can be: enable, suspend or remove. Remove will remove the user and purge all data! You will then need
              # to remove the entry block here after the user has been removed from the cluster.
              status: enable
              key_type: s3
              quota:
                    user:
                        # Allows you to temporarily disable and then later re-enable without have to change the quota amounts
                        status: disable
                        size: -1
                        objects: -1
                    buckets:
                        - name: fill_test
                          # Allows you to temporarily disable and then later re-enable without have to change the quota amounts
                          status: disable
                          size: -1
                          objects: -1
              zones:
              # buckets are for the initial bucket to create if desired. The first 3 users here are system users so we create the buckets.
              # The remaining users are real users and there are no buckets unless we know them and want to place them here
              buckets:
                  - fill_test
            - uid: cepheus_log_admin
              name: "Ceheus Log Admin"
              admin_caps: ""
              access_key: ""
              secret_key: ""
              email: ""
              max_buckets: 0
              status: enable
              key_type: s3
              quota:
                    user:
                        # Allows you to temporarily disable and then later re-enable without have to change the quota amounts
                        status: disable
                        size: -1
                        objects: -1
                    buckets:
                        - name: cepheus_logs
                          # Allows you to temporarily disable and then later re-enable without have to change the quota amounts
                          status: disable
                          size: -1
                          objects: -1
              zones:
              buckets:
                  - cepheus_logs
            - uid: tester
              name: Tester
              admin_caps: "usage=read; user=read; bucket=*"
              access_key: ""
              secret_key: ""
              email: ""
              max_buckets: 3
              status: enable
              key_type: s3
              quota:
                    user:
                        # Allows you to temporarily disable and then later re-enable without have to change the quota amounts
                        status: enable
                        size: 10
                        objects: -1
                        # NOTE: buckets are missing for quotas here just to show it can
              zones:
        federated:
            enable: false
            # master_zone must match one of the vip names.
            master_zone: dev
            master_zone_port: 8080
            # Region represent cluster (datacenter)
            regions:
                - *data_center
            # Zones represent network tiers
            zones:

    osd:
        journal:
            # Production may be something like 20000
            size: 3000
        rebalance: false
        niceness: -10
        crush:
            update: false # true for production...
            update_on_start: true # false for production...
            chooseleaf_type: 1
        device: dev
        devices:
            - device: sdb
              data_type: hdd
              journal: sdf
              journal_type: ssd
              encrypted: false
            - device: sdc
              data_type: hdd
              journal: sdf
              journal_type: ssd
              encrypted: false
            - device: sdd
              data_type: hdd
              journal: sdf
              journal_type: ssd
              encrypted: false
            - device: sde
              data_type: hdd
              journal: sdf
              journal_type: ssd
              encrypted: false
