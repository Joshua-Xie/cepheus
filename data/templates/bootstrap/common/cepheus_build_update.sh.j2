#!/bin/bash
#
# Author: Chris Jones <chris.jones@lambdastack.io>
# Copyright 2017, LambdaStack
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# NB: The initial build process: Chef Server and Chef Clients are installed and setup along with everything else.
# NB: The update process: Chef Server and Chef Client are NOT automatically updated but cookbooks and other supporting files are!

set -e

# Force a new clone
REPO_ROOT={{ cache.repo }}
REPO_ROOT_FILES={{ cache.base }}

WORK_DIR={{ build.work_dir }}
DATE=$(date +%Y-%m-%d_%H%M%S)
VERSION={{ version }}

CLUSTER={{ data_center }}
CEPH_CHEF_BOOTSTRAP={{ bootstrap.name }}

# Change to whatever owner/group you like
OWNER={{ primary_user.name }}
GROUP={{ primary_user.group }}

if [[ -d $WORK_DIR ]]; then
  rm -rf $WORK_DIR
fi

# Pulls down the github.com/cepheus-io/cepheus-staging repo to $WORK_DIR/cepheus-staging
# NOTE: It will prompt for your credentials by default
# Instead of tokens/keys - enforce temp cache of credentials - change it later if desired

# This is OK since only cloning/pulling. It only caches the credentials long enough to pull in all of the repos.
git config --global credential.helper cache
git config --global credential.helper 'cache --timeout=25'

# IMPORTANT: NEVER push ONLY pull/clone
# Pull down (public) cepheus (cepheus-public-staging)

{% for item in staging.process %}
# {{ item.comment }}
{%- if item.command %}
{{ item.command }}
{% else %}
git clone -b {{ item.branch }} {{ item.repo }} {{ item.output }}
{%- endif %}

{%- endfor %}

# If opscode does not exist then Chef has not been installed yet and this is a fresh build.
{% if primary_user.backup.enable %}
if [[ -d "/opt/opscode/" ]]; then
    if [[ -d $REPO_ROOT ]]; then
        {% if primary_user.backup.remote.enable %}
        # Call the backup and rsync script
        source $REPO_ROOT/cepheus_backup_rsync.sh $DATE $VERSION
        {% else %}
        # Call the backup
        source $REPO_ROOT/cepheus_backup_no_rsync.sh $DATE $VERSION
        {% endif %}
    fi
fi
{% endif %}

# Copy internal cepheus over staging
cp -r $WORK_DIR/cepheus/* $WORK_DIR/cepheus-staging/

# Update the scripts themselves
# cp -p $WORK_DIR/cepheus-data/*.sh $HOME/.

# Remove script from data area since it was copied to $OWNER/$GROUP home above
# rm -f $WORK_DIR/cepheus-data/cepheus_*.sh

# Copy data over staging
cp -r $WORK_DIR/cepheus-data/* $WORK_DIR/cepheus-staging/

# Remove unneeded directories and files
rm -rf $WORK_DIR/cepheus
rm -rf $WORK_DIR/cepheus-data
# rm -f $WORK_DIR/cepheus-staging/environments/*.json

echo "====> Clean staging process directories..."
# Clean up staging process directories if they exist
# NOTE: Important - Do this BEFORE moving data into directories on bootstrap since *all* is used!!

set +e
# Only if Chef has been installed which means ansible is loaded and host files exist
if [[ -d "/opt/opscode/" ]]; then
    # Nodes may not exist during initial build so it's ok to fail...
    ansible all -m shell -a "sudo rm -rf $REPO_ROOT"
    ansible all -m shell -a "sudo rm -rf $REPO_ROOT_FILES"
fi
set -e

echo "====> Building update..."
# Build new ceph-host environment and replace existing version

# Create dir even though it may already exist
sudo mkdir -p $REPO_ROOT
sudo chown -R $OWNER:$GROUP $REPO_ROOT

if [[ $(ls -A {{ cache.repo }}) ]]; then
    sudo rm -rf $REPO_ROOT/*
fi

cp -r $WORK_DIR/cepheus-staging/* $REPO_ROOT
sudo chown -R $OWNER:$GROUP $REPO_ROOT


####
# NB: This process may seem a little odd at first depending on the environment. For Vagrant the commands above 'cp'
# to the cepheus-staging because it pulls in the dependencies via the bootstrap_prereqs.sh. However, other environments
# may pull in from internal github or artifactory or something else. So, this makes sure it gets filled. May change later.
# Update {{ cache.base }}/ (supporting files - dependencies)
sudo mkdir -p $REPO_ROOT_FILES
sudo chown -R $OWNER:$GROUP $REPO_ROOT_FILES
#
if [[ $(ls -A $REPO_ROOT_FILES) ]]; then
    sudo rm -rf $REPO_ROOT_FILES/*
fi
#
cp -r $WORK_DIR/cepheus-files/* $REPO_ROOT_FILES
sudo chown -R $OWNER:$GROUP $REPO_ROOT_FILES
#
####


# Build...
echo "====> Building from templates..."

# Build from template
{%- for item in build_update_files %}
$REPO_ROOT/data/templates/template_engine -d $REPO_ROOT/data/$BUILD_LOCATION/$BUILD_DATA_CENTER/build.yaml -i {{ item.input }} -o {{ item.output }}
# NB: Pass through template_engine twice
$REPO_ROOT/data/templates/template_engine -d $REPO_ROOT/data/$BUILD_LOCATION/$BUILD_DATA_CENTER/build.yaml -i {{ item.output }} -o {{ item.output }}
{% endfor %}

# Sync $HOME/cepheus with {{ cache.repo }}
sudo rm -rf $HOME/cepheus/*
cp -rp $REPO_ROOT/* $HOME/cepheus

# Add the dependency cookbooks from the file cache
echo "====> Checking on dependency for cookbooks..."
cp $REPO_ROOT_FILES/cookbooks/*.tar.gz $HOME/cepheus/cookbooks

# Make sure it's {{ primary_user.name }} as owner:group
sudo chown -R $OWNER:$GROUP $HOME/cepheus

cd $HOME/cepheus/cookbooks && ls -1 *.tar.gz | xargs -I% sudo tar xvzf %
cd $HOME/cepheus/cookbooks && sudo rm -f *.tar.gz

# Make sure it's {{ primary_user.name }} as owner:group
sudo chown -R $OWNER:$GROUP $HOME/cepheus

# Override ceph-chef with ceph-chef-staging. This will override an existing 'ceph-chef' package that may already exist.
mkdir -p $HOME/cepheus/cookbooks/ceph-chef
cp -r $WORK_DIR/ceph-chef-staging/* $HOME/cepheus/cookbooks/ceph-chef

# Update chef info
knife cookbook upload -a
cd $HOME/cepheus/roles && knife role from file *.json
cd $HOME/cepheus/environments && knife environment from file {{ environment }}.json

# Add Chef tags
# NOTE: run_list *set* replaces any run_list that may have already been there before
knife node run_list set {{ bootstrap.name }} 'role[ceph-bootstrap]'
knife node environment set {{ bootstrap.name }} {{ environment }}
knife tag create {{ bootstrap.name }} 'ceph-bootstrap'

sudo chef-client -o 'recipe[cepheus::build-env]'

# Update Chef tags
/ceph-host/bootstrap/data/environment/common/scripts/chef_tags_delete.sh
/ceph-host/bootstrap/data/environment/common/scripts/chef_tags_create.sh

# Make sure the production environment is always set
/ceph-host/bootstrap/data/environment/common/scripts/chef_environment_nodes.sh {{ environment }}

# Update run_lists
/ceph-host/bootstrap/data/environment/common/scripts/chef_runlists_update.sh

# Update bootstrap node
##################
# Tmp comment out for testing...
sudo chef-client

logger -t CepheusBackup "Cepheus Chef Server updated to latest packages - $VERSION-$DATE"

# Make sure tmp files are present for cookbooks
# Hack around gem issue - one of Chef's bad things...
# NB: Fix in future - Should only run these ansible runs *IF* nodes exists but for now it's ok if they fail on intial run...
set +e
{% for item in ceph_chef.gems %}
ansible all -m copy -a "src=$REPO_ROOT_FILES/gems/{{ item.name }}-{{ item.version }}.gem dest=/tmp/{{ item.name }}-{{ item.version }}.gem owner=$OWNER"
{% endfor %}

# Backup ALL important configs, keys etc
# I have seen this sometimes hang - maybe due to doing a number of crush map gets which means it needs to be
# optimized to only get configs for each of the node groups since the configs are the same for all in that group and
# then maybe store the backup on the boostrap as well as the primary node in the given group. Until then, it's OK to ctrl-c out of it.

ansible {{ inventory.ceph.backup.primary.mon }},{{ inventory.ceph.backup.primary.osd }},{{ inventory.ceph.backup.primary.rgw }} -m shell -a "sudo /etc/ceph/scripts/ceph_backup_files.sh $VERSION $DATE"
set -e
##################

# Now you can run chef-client on the nodes

# Remove tmp working directory
rm -rf $WORK_DIR

echo "====> cepheus_build_update.sh Success!"
echo
