{
  {#- NOTE: This template goes through a multi-phase process where the first pass pulls from one data file and  #}
  {#- and the second from another data file. The yaml data is found in /data/private or /data/public depending on option #}
  "name": "{{ environment }}",
  "json_class": "Chef::Environment",
  "description": "Data to build out {{ data_center }} Cepheus Cluster",
  "cookbook_versions": {},
  "chef_type": "environment",
  "override_attributes": {
    "ceph": {
        "cluster": "{{ ceph.cluster }}",
        "owner": "{{ ceph.owner }}",
        "group": "{{ ceph.group }}",
        "mode": {{ ceph.mode }},
        "repo": {
          "create": {%- if ceph_repo_create == True %} true, {% else %} false, {%- endif %}
          "version": {
            "name": "{{ ceph.name }}",
            "number": "{{ ceph.version }}",
            "branch": "{{ ceph_repo.branch }}",
            "revision": "{{ ceph_repo.revision }}",
            "arch": "{{ ceph_repo.arch }}"
          }
        },
        "version": "{{ ceph.name }}",
        "branch": "{{ ceph_repo.branch }}",
        "revision": "{{ ceph_repo.revision }}",
        "el_version": "el7",
        "repo_url": "http://download.ceph.com",
        "system": {
            "sysctls": [
            {%- for item in system.sysctl.sysctls %}
                "{{ item }}"{%- if not loop.last %},{% endif %}
            {%- endfor %}
            ]
        },
        "mgr": {
          "enable": {%- if ceph.mgr.enable == True %} true {% else %} false {%- endif %}
        },
        "config": {
          {#- "NOTE": "This section is pure key/value. Meaning, the key and value are added to the given location in ceph.conf.", #}
          "global": {
            "rgw override bucket index max shards": {{ ceph.radosgw.bucket_shards }}
          },
          "mon": {
            {#- 'mon allow pool delete' = false *AFTER* the cluster is stable. Keeps someone from deleting pools :) #}
            "mon osd full ratio": 0.95,
            "mon osd nearfull ratio": 0.85,
            "mon pg warn max per osd": 0,
            "mon pg warn max object skew": 10,
            "mon osd min down reporters": 3,
            "mon osd down out interval": 600,
            "clock drift allowed": 0.15
          },
          "radosgw": {
            "cache max file size": 20000000
          }
        },
        "mon": {
          "port": {{ ceph.mon.port }},
          "niceness": {{ ceph.mon.niceness }},

          {#- NOTE: WIP - TODO: Remove unused code from CEPH portions! #}

          {#- NB: Need to remove the bond portion of here and the recipe #}
          "bond": {
            "enable": {%- if bond.enable == True %} true, {% else %} false, {%- endif %}
            "name": "{{ bond.name }}",
            "type": "Bond",
            "mtu": {{ bond.mtu }},
            "interfaces": ["{{ bond.interfaces|join('","') }}"],
            "options": "{{ bond.options }}",
            "nm_controlled": "no"
          }
        },
        "radosgw": {
          "port": {{ ceph.radosgw.default_port }},
          "default_url": "{{ ceph.radosgw.default_url }}",
          "dns_name" : "{{ domain }}",
          "debug": {
            "logs": {
              "level": {{ ceph.radosgw.debug.logs.level }},
              "enable": {%- if ceph.radosgw.debug.logs.enable == True %} true {% else %} false {%- endif %}
            }
          },
          "logs": {
            "ops": {%- if ceph.radosgw.logs.ops.enable == True %} true, {% else %} false, {%- endif %}
            "usage": {%- if ceph.radosgw.logs.usage.enable == True %} true {% else %} false {%- endif %}
          },
          "rgw_webservice": {
            "enable": {%- if ceph.radosgw.rgw_webservice.enable == True %} true, {% else %} false, {%- endif %}
            "port": {{ ceph.radosgw.rgw_webservice.port }},
            "user": "{{ ceph.radosgw.rgw_webservice.user }}"
          },
          "gc": {
            "max_objects": {{ ceph.radosgw.gc.max_objects }},
            "obj_min_wait": {{ ceph.radosgw.gc.obj_min_wait }},
            "processor_max_time": {{ ceph.radosgw.gc.processor_max_time }},
            "processor_period": {{ ceph.radosgw.gc.processor_period }}
          },
          "keystone": {
            "auth": {%- if ceph.radosgw.keystone.enable == True %} true, {% else %} false, {%- endif %}
            "accepted_roles": "{{ ceph.radosgw.keystone.accepted_roles }}",
            "token_cache_size": {{ ceph.radosgw.keystone.token_cache_size }},
            "revocation_interval": {{ ceph.radosgw.keystone.revocation_interval }},
            "admin": {
                "token": "{{ ceph.radosgw.keystone.admin.token}}",
                "url": "{{ ceph.radosgw.keystone.admin.url }}",
                "port": {{ ceph.radosgw.keystone.admin.port }}
            }
          },
          "users": [
            {%- for user in ceph.radosgw.users %}
              {"uid": "{{ user.uid }}", "name": "{{ user.name }}", "email": "{{ user.email }}", "admin_caps": "{{ user.admin_caps }}", "access_key": "{{ user.access_key }}", "secret_key": "{{ user.secret_key }}", "max_buckets": {{ user.max_buckets }}, "status": "{{ user.status }}", "key_type": "{{ user.key_type }}", "quota": {"user": { "status": "{{ user.quota.user.status }}", "size": {{ user.quota.user.size }}, "objects": {{ user.quota.user.objects }} }, "buckets": [{%- for bucket in user.quota.buckets %}{"name": "{{ bucket.name }}", "status": "{{ bucket.status }}", "size": {{ bucket.size }}, "objects": {{ bucket.objects }}}{%- if not loop.last %},{% endif %}{%- endfor %}]}, "zones": [{%- for zone in user.zone %}"{{ zone }}"{%- if not loop.last %},{% endif %}{% endfor %}], "buckets": [{%- for bucket in user.buckets %}{"name": "{{ bucket.name }}", "acl": "{{ bucket.acl }}"}{%- if not loop.last %},{% endif %}{% endfor %}]}{%- if not loop.last %},{% endif %}
            {%- endfor %}
          ],
          "rgw_num_rados_handles": {{ ceph.radosgw.rados_handles }},
          "civetweb_num_threads": {{ ceph.radosgw.civetweb_threads }},

          {#- NOTE: WIP - TODO: Remove unused code from CEPH portions! #}

          "bond": {
            "enable": {%- if bond.enable == True %} true, {% else %} false, {%- endif %}
            "name": "{{ bond.name }}",
            "type": "Bond",
            "mtu": {{ bond.mtu }},
            "interfaces": ["{{ bond.interfaces|join('","') }}"],
            "options": "{{ bond.options }}",
            "nm_controlled": "no"
          }
        },
        "osd": {
          {#- Move this data to the seed yamls if we need to make them more dynamic for different clusters. #}
          "devices": [
              {%- for item in ceph.osd.devices %}
                {"data": "{{ ceph.osd.device }}{{ item.device }}", "data_type": "{{ item.data_type }}", "journal": "{{ ceph.osd.device }}{{ item.journal }}", "journal_type": "{{ item.journal_type }}", "encrypted": {% if item.encrypted == True %}true{% else %}false{%- endif %}}{%- if not loop.last %},{% endif %}
              {%- endfor %}
          ],
          "crush": {
            "update": {%- if ceph.osd.crush.update_crush == True %} true, {% else %} false, {%- endif %}
            "update_on_start": {%- if ceph.osd.crush.update_on_start == True %} true, {% else %} false, {%- endif %}
            "chooseleaf_type": {{ ceph.osd.crush.chooseleaf_type }}
          },
          {#- The add and remove should reflect the same structure as devices above and should be used for maintenance only! #}
          "add": [],
          "remove": [],
          "niceness": {{ ceph.osd.niceness }},
          {#- "encrypted": false, #}
          "rebalance": {%- if ceph.osd.rebalance == True %} true, {% else %} false, {%- endif %}
          "size": {
            "max": 3,
            "min": 2
          },
          "journal": {
            {#- Change the journal size for the prod SSD #}
            {#- Calculate min size (if you have space then use it): 2 * (#OSDs in node * 100MB/s) * 5 (default sync interval) = 2 * (12 * 100) * 5 = 12GB #}
            "size": {{ ceph.osd.journal.size }}
          }
        },
        "pools": {
          "active": [
            "{{ ceph.pools.active|join('", "') }}"
          ],
          "version": {{ ceph.pools.version }},
          "crush": {
            "rule": {{ ceph.pools.crush_rule }}
          },
          "erasure_coding": {
            "profiles": [
              {"profile": "{{ ceph.ec.profile }}", "directory": "{{ ceph.ec.directory }}", "plugin": "{{ ceph.ec.plugin }}", "force": true, "technique": "{{ ceph.ec.technique }}", "ruleset_root": "{{ ceph.ec.ruleset_root }}", "ruleset_failure_domain": "{{ ceph.ec.ruleset_failure_domain }}", "key_value": {"k": {{ ceph.ec.k }}, "m": {{ ceph.ec.m }}}}
            ]
          },
          "radosgw": {
            "federated": {
              "enable": {%- if ceph.radosgw.federated.enable == True %} true, {% else %} false, {%- endif %}
              "multisite_replication": false,
              {#- Instance name MUST match VIPs name and Backend/Server instance variables. IF Federation is not used then Backend/Server instance variable should be empty. #}
              "zone_instances": [
                {%- for item in adc.vip.vips %}
                  {%- if item.type == "rgw" %}
                  {"name": "{{ item.name }}", "port": {{ item.backend_port }}, "region": "{{ item.region }}", "url": "{{ item.url }}", "rgw_thread_pool": {{ item.rgw_thread_pool }}, "handles": {{ item.handles }}, "threads": {{ item.threads }}}{%- if not loop.last %},{% endif %}
                  {%- endif %}
                {%- endfor %}
              ],
              "regions": ["{{ ceph.radosgw.federated.regions|join('", "') }}"],
              "enable_regions_zones": {%- if ceph.radosgw.federated.enable_regions_zones == True %} true, {% else %} false, {%- endif %}
              "master_zone": "{{ ceph.radosgw.federated.master_zone }}",
              "master_zone_port": {{ ceph.radosgw.federated.master_zone_port }}
            },
            {#- The data_percent below should equal 100%. If Federated then the calculation will take those pools into account. #}
            {#- NOTE: IMPORTANT - ONLY 'buckets' (data) can be erasure coded. Others need to be replicated. #}
            "pools": [
                {%- for item in ceph.pools.radosgw %}
                  {"name": "{{ item.name }}", "data_percent": {{ "{:,.2f}".format(item.data_percent) }}, "type": "{{ item.type }}", "profile": "{{ item.profile }}", "crush_ruleset": {{ item.crush_ruleset }}, "crush_ruleset_name": "{{ item.crush_ruleset_name }}"}{%- if not loop.last %},{% endif %}
                {%- endfor %}
            ],
            {#- If federated then the settings below apply to the federated pools else the non-federated pools. #}
            "settings": {
              "pg_num": {{ ceph.radosgw.settings.pg_num }},
              "pgp_num": {{ ceph.radosgw.settings.pgp_num }},
              "options": "{{ ceph.radosgw.settings.options }}",
              "force": {%- if ceph.radosgw.settings.force == True %} true, {% else %} false, {%- endif %}
              "calc": {%- if ceph.radosgw.settings.calc == True %} true, {% else %} false, {%- endif %}
              "size": {{ ceph.radosgw.settings.size }},
              "crush_ruleset": {{ ceph.radosgw.settings.crush_ruleset }},
              "chooseleaf": "{{ ceph.radosgw.settings.chooseleaf }}",
              "chooseleaf_type": {{ ceph.radosgw.settings.chooseleaf_type }},
              "type": "{{ ceph.radosgw.settings.type }}",
              "nodes_per_rack": {{ ceph.radosgw.settings.nodes_per_rack }}
            },
            "remove": {
              "names": []
            }
          },
          "pgs": {
            "calc": {
              "total_osds": {{ ceph.pgs.calc.total_osds }},
              "target_pgs_per_osd": {{ ceph.pgs.calc.target_pgs_per_osd }},
              "min_pgs_per_pool": {{ ceph.pgs.calc.min_pgs_per_pool }},
              "replicated_size": {{ ceph.pgs.calc.replicated_size }},
              "erasure_size": {{ ceph.pgs.calc.erasure_size }}
            },
            "num": {{ ceph.pgs.num }}
          }
        },
        "restapi": {
          "url": "{{ ceph.restapi.url }}",
          "ip": "{{ ceph.restapi.ip }}",
          "port": {{ ceph.restapi.port }}
        },
        "network": {
          "public": {
            "interface": "{{ network.public.interface }}",
            "cidr": [
              "{{ network.public.cidr|join('", "') }}"
            ],
            "mtu": {{ network.public.mtu }}
          },
          "cluster": {
            "interface": "{{ network.cluster.interface }}",
            "gateway_enable": {%- if network.gateway_enable == True %} true, {% else %} false, {%- endif %}
            "cidr": [
              "{{ network.cluster.cidr|join('", "') }}"
            ],
            "route": {
              "cidr": "{{ network.cluster.route.cidr }}"
            },
            "mtu": {{ network.cluster.mtu }}
          }
        }
    },
    "cepheus": {
      "environment": "{{ environment }}",
      "method": "{{ method }}",
      "repo": {
        {#- Pins the current version of ceph or any other packaged in the array at the moment of install. #}
        "packages": [
          {"name": "", "version": "", "pin": true}
        ]
      },
      "development": {
          "enabled": {%- if development.enable == True %} true, {% else %} false, {%- endif %}
          "user": "{{ development.user }}"
      },
      "cron": {
        "logs": {
            "enable": {%- if cron.logs.enable == True %} true, {% else %} false, {%- endif %}
            "radosgw": {
                {#- Builds a cron file that injects the Cepheus logs into RGW and allows the normal logrotate to remove the old logs #}
                {#- This allows any process with the 'log' credentials to extract the logs for use in other products like Splunk #}
                "uid": "{{ cron.logs.radosgw.uid }}",
                "name": "{{ cron.logs.radosgw.name }}",
                "access_key": "{{ cron.logs.radosgw.access_key }}",
                "secret_key": "{{ cron.logs.radosgw.secret_key }}",
                "endpoint": "{{ cron.logs.radosgw.endpoint }}",
                "port": {{ cron.logs.radosgw.port }},
                "bucket": "{{ cron.logs.radosgw.bucket }}"
            },
            "patterns": [
            {%- for item in cron.logs.patterns %}
                {"directory": "{{ item.directory }}", "pattern": "{{ item.pattern }}", "bucket": {%- if item.bucket %}"{{ item.bucket }}"{% else %}"{{ cron.logs.radosgw.bucket }}"{%- endif %}}{%- if not loop.last %},{% endif %}
            {%- endfor %}
            ]
        },
        "radosgw": {
            "stats": {
                "user": "{{ primary_user.name }}",
                "minute": "{{ cron.radosgw.stats.minute }}",
                "command": "{{ cron.radosgw.stats.command }}"
            }
        }
      },
      "bootstrap": {
        "name": "{{ bootstrap.name }}",
        "env": "{{ bootstrap.env }}",
        "interfaces": [
        {%- for item in bootstrap.interfaces %}
          {"name": "{{ item.device }}", "ip": "{{ item.ip }}", "netmask": "{{ item.netmask }}", "gateway": "{{ item.gateway }}", "mac": "{{ item.mac }}"}{%- if not loop.last %},{% endif %}
        {%- endfor %}
        ]
      },

      "adc": {
        "enable": {%- if adc.enable == True %} true, {% else %} false, {%- endif %}
        "tag": "ceph-adc",
        "interface": "{{ adc.interface }}",
        "bond": {
          "enable": {%- if bond.enable == True %} true, {% else %} false, {%- endif %}
          "name": "{{ bond.name }}",
          "type": "Bond",
          "mtu": {{ bond.mtu }},
          "interfaces": ["{{ bond.interfaces|join('","') }}"],
          "options": "{{ bond.options }}",
          "nm_controlled": "no"
        },
        "stats": {
            "enable": {%- if adc.haproxy.stats.enable == True %} true, {% else %} false, {%- endif %}
            "user": "{{ adc.haproxy.user }}",
            "passwd": "{{ adc.haproxy.passwd }}",
            "port": {{ adc.haproxy.port }}
        },
        "connections": {
          "max": {{ adc.haproxy.max_connections }},
          "balance": "{{ adc.haproxy.balance }}"
        },

        "ssl": {
          "path": "/etc/ssl/private"
        },
        "vip": {
          "prefix": "{{ adc.vip.prefix }}",
          "netmask": "{{ adc.vip.netmask }}",
          "cidr": {{ adc.vip.cidr }},
          "port": {
            "ssl": 443,
            "non_ssl": 80
          },
          "rgw_webservice": {
            "port": {{ adc.rgw_webservice.vip.port }},
            "ip": "{{ adc.rgw_webservice.vip.ip }}"
          }
        },
        {#- backend_port is used in the backend section below for Federated only. #}
        "vips": [
        {%- for item in adc.vip.vips %}
          {"name": "{{ item.name }}", "haproxy": {%- if item.haproxy == True %} true {% else %} false {%- endif %}, "ip": "{{ item.ip }}", "cidr": {{ adc.vip.cidr }}, "interface": "{{ item.interface }}", "backend_label": "{{ item.backend_label }}", "ssl": {%- if item.ssl == True %} true {% else %} false {%- endif %}, "cert": "{{ item.cert }}", "ssl_files": ["{{ item.ssl_files|join('","') }}"], "url": "{{ item.url }}"}{%- if not loop.last %},{% endif %}
        {%- endfor %}
        ],
        {#- VIPS are 10.121.16.16/28 range (.17 - .30). Advertised via BGP or Static+BFD Beacon. #}
        "bgp": {
          "enable": {%- if adc.bgp.enable == True %} true, {% else %} false, {%- endif %}
          "asn": {{ adc.bgp.asn }},
          "interface": "{{ adc.bgp.interface }}",
          "roles": [
          {%- for item in adc.bgp.roles %}
            {"name": "{{ item.name }}", "role": "{{ item.role }}"}{%- if not loop.last %},{% endif %}
          {%- endfor %}
          ],
          "peers": [
            {%- for item in adc.bgp.peers %}
              {"name": "{{ item.name }}", "label": "{{ item.label }}", "ip": "{{ item.ip }}"}{%- if not loop.last %},{% endif %}
            {%- endfor %}
          ]
        },
        "backend": {
          {#- NOTE: IF Federated then VIPs 'name' variable, Backend/Servers 'instance' variable and Ceph/Pools/Radosgw/Federated/Zone_Instances 'name' *MUST* be the same value for the lookups to work. #}
          {#- IF Federated is NOT used then leave 'instance' variable name below empty. #}
          "servers": [
          {%- if ceph.radosgw.federated.enable %}
          {%- for vip in vip.vips %}
            {%- set rowloop = loop %}
            {%- if vip.type == "rgw" %}
              {%- for item in adc.backend.servers %}
                {"name": "{{ item.name }}", "instance": "{{ vip.name }}", "type": "{{ item.type }}", "weight": "{{ item.weight }}", "port": {{ vip.backend_port }}, "ip": "{{ item.ip }}", "options": "{{ item.options }}"}{%- if not rowloop.last %},{% elif not loop.last %},{% endif %}
              {%- endfor %}
            {%- endif %}
          {%- endfor %}
          {%- else %}
          {%- for item in adc.backend.servers %}
            {"name": "{{ item.name }}", "instance": "", "type": "{{ item.type }}", "weight": "{{ item.weight }}", "port": {{ item.port }}, "ip": "{{ item.ip }}", "options": "{{ item.options }}"}{%- if not loop.last %},{% endif %}
          {%- endfor %}
          {%- endif %}
          ]
        },

        "keepalived": {
          "enable": {%- if adc.keepalived.enable == True %} true, {% else %} false, {%- endif %}
          "passwd": "{{ adc.keepalived.passwd }}",
          "checks": {%- if adc.keepalived.checks == True %} true, {% else %} false, {%- endif %}
          "servers": [
          {%- for item in adc.keepalived.servers %}
            {"name": "{{ item.name }}", "weight": "{{ item.weight }}", "interface": "{{ item.interface }}", "priority": {{ item.priority }}}{%- if not loop.last %},{% endif %}
          {%- endfor %}
          ]
        }
      },

      "chef": {
        "owner": "{{ chef.owner }}",
        "group": "{{ chef.group }}"
      },
      "security": {
        "sshd": {
          "permit_root_login": "{{ security.sshd.permit_root_login }}",
          "login_grace_time": "{{ security.sshd.login_grace_time }}",
          "max_auth_tries": {{ security.sshd.max_auth_tries }},
          "max_sessions": {{ security.sshd.max_sessions }},
          "banner": "{{ security.sshd.banner }}"
        },
        "firewall": {
          "enable": {%- if security.firewall.enable == True %}true,{% else %}false,{%- endif %}
          "zone": "{{ security.firewall.zone }}",
          "use": "{{ security.firewall.use }}",
          "rules": [
          {%- for item in security.firewall.rules %}
            {"name": "{{ item.name }}", "type": "{{ item.type }}", "zone": "{{ item.zone }}", "permanent": {%- if item.permanent == True %} true{% else %} false{%- endif %}, "roles": ["{{ item.roles|join('","') }}"], "rules": ["{{ item.rules|join('","') }}"]}{%- if not loop.last %},{% endif %}
          {%- endfor %}
          ],
          "interfaces": [
            {
              "name": "public",
              "ports": [
                {"role": "ceph-bootstrap", "open": [{"port": 123, "protocol": "udp"}, {"port": 80, "protocol": "tcp"}, {"port": 443, "protocol": "tcp"}, {"port": 67, "protocol": "udp"}, {"port": 69, "protocol": "udp"}, {"port": 21, "protocol": "tcp"}, {"port": 4011, "protocol": "udp"}, {"port": 53, "protocol": "udp"}], "ranges": [{"start": 25150, "end": 25152, "protocol": "tcp"}]},
                {"role": "ceph-mon", "open": [{"port": 6789, "protocol": "tcp"}], "ranges": [{"start": 0, "end": 0, "protocol": "tcp"}]},
                {"role": "ceph-osd", "open": [], "ranges": [{"start": 6800, "end": 6872, "protocol": "tcp"}]},
                {"role": "ceph-rgw", "open": [8080], "ranges": [{"start": 0, "end": 0, "protocol": "tcp"}]},
                {"role": "ceph-restapi", "open": [{"port": 5080, "protocol": "tcp"}], "ranges": [{"start": 0, "end": 0, "protocol": "tcp"}]},
                {"role": "ceph-admin", "open": [], "ranges": [{"start": 0, "end": 0, "protocol": "tcp"}]},
                {"role": "ceph-mds", "open": [], "ranges": [{"start": 6800, "end": 6872, "protocol": "tcp"}]},
                {"role": "ceph-rbd", "open": [], "ranges": [{"start": 0, "end": 0, "protocol": "tcp"}]},
                {"role": "haproxy", "open": [{"port": 80, "protocol": "tcp"}, {"port": 443, "protocol": "tcp"},{"port": 1936, "protocol": "tcp"}], "ranges": [{"start": 0, "end": 0, "protocol": "tcp"}]},
                {"role": "keepalived", "open": [{"port": 112, "protocol": "tcp"}], "ranges": [{"start": 0, "end": 0, "protocol": "tcp"}]}
              ]
            },
            {
              "name": "cluster",
              "ports": [
                {"role": "ceph-osd", "open": [], "ranges": [{"start": 6800, "end": 6872, "protocol": "tcp"}]}
              ]
            }
          ]
        }
        {#- "NOTE1": "NOTE: Firewall open ports are accumulative for each node based on it's role. Role must match ceph-chef tags.", #}
        {#- "NOTE2": "NOTE: Range start = 0 then range is skipped else put in exact ranges.", #}
        {#- "NOTE3": "NOTE: OSDs start at 6800 and each OSD uses at least 3 ports. The end number should be high enough to account for this. MDS should match OSD.", #}
        {#- "NOTE4": "NOTE: If you run multiple instances of RGW then keep the port count in mind." #}
      },
      "system": {
        "scheduler": {
            "device": {
                "enable": {%- if system.scheduler.device.enable == True %}true,{% else %}false,{%- endif %}
                "type": "{{ system.scheduler.device.type }}",
                "ceph": {
                    "class": "{{ ceph.system.class }}",
                    "priority": {{ ceph.system.priority }}
                },
                "devices": [
                    {%- for item in ceph.osd.devices %}
                      "{{ item.device }}"{%- if not loop.last %},{% endif %}
                    {%- endfor %}
                ]
            }
        },
        "sysctl": {
            "enable": {%- if system.sysctl.enable == True %}true,{% else %}false,{%- endif %}
            "sysctls": [
                {%- for item in system.sysctl.sysctls %}
                    "{{ item }}"{%- if not loop.last %},{% endif %}
                {%- endfor %}
            ]
        }
      },
      "ipmi": {
        "user": "{{ ipmi.user }}",
        "passwd": "{{ ipmi.passwd }}"
        {#- "NOTE": "password of vbox is: $6$Salt$xvQkYaQ4urNWmnjpinAZSR/ZOaRy/aacKh4j18ayq/.mswLqleFZI5zaD1BCg2Fdzy1BjpBv9VIgVgt6YoA8T0" #}
      },

      "servers": [
          {%- for item in inventory.nodes %}
            {"name": "{{ item.name }}",
            "roles": [{%- for role in item.roles %}"{{role}}"{%- if not loop.last %},{% endif %}{%- endfor %}],
            "profile": "{{ item.profile }}",
            "interfaces": [
                {%- for interface in item.interfaces %}
                    {"profile": "{{ interface.profile }}", "interface": "{{ interface.device }}", "mac": "{{ interface.mac }}", "ip": "{{ interface.ip }}", "netmask": "{{ interface.netmask }}", "gateway": "{{ interface.gateway }}", "mtu": {{ interface.mtu }}}{%- if not loop.last %},{% endif %}
                {%- endfor %}
            ]}{%- if not loop.last %},{% endif %}
          {%- endfor %}
      ],

      "primary_user": {
        "name": "{{ primary_user.name }}",
        "passwd": "{{ primary_user.passwd }}",
        "passwd_type": "{{ primary_user.passwd_type }}",
        "public_key": "{{ primary_user.public_key }}",
        "private_key": "{{ primary_user.private_key }}",
        "shell": "{{ primary_user.shell }}",
        "sys": {% if primary_user.shell == True %}true{% else %}false{% endif %},
        "groups": ["{{ primary_user.groups|join('","') }}"],
        "roles": ["{{ primary_user.roles|join('","') }}"],
        "comment": "{{ primary_user.comment }}",
        "group": "{{ primary_user.group }}",
        "group_create": {% if primary_user.group_create == True %}true{% else %}false{% endif %},
        "sudo": {%- if primary_user.sudo == True %}true{% else %}false{%- endif %}
      },

      {#- System users #}
      "users": [
          {%- for user in users %}
            {
            "name": "{{ user.name }}",
            "group": "{{ user.group }}",
            "group_create": {% if user.group_create == True %}true{% else %}false{% endif %},
            "comment": "{{ user.comment }}",
            "passwd": "{{ user.passwd }}",
            "passwd_type": "{{ user.passwd_type }}",
            "key": "{{ user.key }}",
            "private_key": "{{ user.private_key }}",
            "shell": "{{ user.shell }}",
            "sys": {% if user.sys == True %}true{% else %}false{% endif %},
            "groups": ["{{ user.groups|join('","') }}"],
            "roles": ["{{ user.roles|join('","') }}"],
            "sudo": {% if user.sudo == True %}true{% else %}false{% endif %},
            "ceph_group": {% if user.ceph_group == True %}true{% else %}false{% endif %}
            }{%- if not loop.last %},{% endif %}
          {%- endfor %}
      ],

      {% if pxe_boot.enable %}
      "pxe_boot": {
        "enable": {%- if pxe_boot.enable == True %}true,{% else %}false,{%- endif %}
        "web_user": "{{ pxe_boot.web_user }}",
        "http_port": {{ pxe_boot.http_port }},
        "pxe_interface": "{{ pxe_boot.interface }}",
        "server": "{{ pxe_boot.server }}",
        "kickstart": {
          "root": {
            "passwd": "{{ pxe_boot.user.root.passwd }}",
            "passwd_type": "{{ pxe_boot.user.root.passwd_type }}",
            "key": "{{ pxe_boot.user.root.key }}"
          },
          "file": {
            "osd": "cepheus_node_rhel_osd.ks",
            "nonosd": "cepheus_node_rhel_nonosd.ks"
          },
          "bootloader": {
            "passwd": "{{ pxe_boot.kickstart.bootloader.passwd }}",
            "passwd_type": "{{ pxe_boot.kickstart.bootloader.passwd_type }}"
          },
        },
        "profiles": [
            {%- for item in pxe_boot.profiles %}
              {"name": "{{ item.name }}", "comment": "{{ item.comment }}"}{%- if not loop.last %},{% endif %}
            {%- endfor %}
        ],
        "dhcp": {
          "shared_network": "{{ pxe_boot.dhcp.shared_network }}",
          "use": "{{ pxe_boot.dhcp.use }}",
          "single": {
            "netmask": "{{ pxe_boot.dhcp.single.netmask }}",
            "gateway": "{{ pxe_boot.dhcp.single.gateway }}"
          },
          "subnets":[
          {%- for item in pxe_boot.dhcp.subnets %}
            {"subnet": "{{ item.subnet }}", "tag": "{{ item.tag }}", "dhcp_range": ["{{ item.dhcp_range|join('","') }}"], "netmask": "{{ item.netmask }}", "gateway": "{{ item.gateway }}"}{%- if not loop.last %},{% endif %}
          {%- endfor %}
          ]
         },
        {#- "NOTE1": "NOTE: Each subnet represents a routable rack so dhcp will need to manage each subnet with the TOR using IP-helper for dhcp requests by nodes in the given rack.", #}
        {#- "NOTE2": "NOTE: You could just have a single subnet for a single L2 span set of racks.", #}
        {#- "NOTE3": "NOTE: /27 for subnet mask of each rack. DNS could be added to each subnet entry above but the global DNS entry below is good enough for this.", #}
        "raid": false,

        "partition_option": "ignoredisk --drives={{ ceph.osd.devices.device|join(',') }}",

        "partitions": [
          {"part": "/boot", "fstype": "xfs", "size": 1024, "options": "--ondisk=sdm"},
          {"part": "/", "fstype": "xfs", "size": 150000, "options": "--ondisk=sdm"},
          {"part": "/var/lib", "fstype": "xfs", "size": 40000, "options": "--ondisk=sdm"},
          {"part": "/opt", "fstype": "xfs", "size": 20000, "options": "--ondisk=sdm"},
          {"part": "swap", "fstype": "swap", "size": 20000, "options": "--ondisk=sdm"}
        ],
        {#- Came back and added RAID1 to the OSD SSDs because of issues in supermicro servers. These journals and OS share SSDs because of design limitations. #}
        "raid_partitions": [
          {"raid": "part raid.01 --size=1024  --ondisk=sdm"},
          {"raid": "part raid.02 --size=150000 --ondisk=sdm"},
          {"raid": "part raid.03 --size=40000 --ondisk=sdm"},
          {"raid": "part raid.04 --size=20000 --ondisk=sdm"},
          {"raid": "part raid.05 --size=20000 --ondisk=sdm"},
          {"raid": "part raid.06 --size=1024  --ondisk=sdn"},
          {"raid": "part raid.07 --size=150000 --ondisk=sdn"},
          {"raid": "part raid.08 --size=40000 --ondisk=sdn"},
          {"raid": "part raid.09 --size=20000 --ondisk=sdn"},
          {"raid": "part raid.10 --size=20000 --ondisk=sdn"},
          {"raid": "raid /boot      --level=RAID1 --device=md0 --fstype=xfs  raid.01 raid.06"},
          {"raid": "raid /          --level=RAID1 --device=md1 --fstype=xfs  raid.02 raid.07"},
          {"raid": "raid /var/lib   --level=RAID1 --device=md2 --fstype=xfs  raid.03 raid.08"},
          {"raid": "raid /opt       --level=RAID1 --device=md3 --fstype=xfs  raid.04 raid.09"},
          {"raid": "raid swap       --level=RAID1 --device=md4 --fstype=swap raid.05 raid.10"}
        ],
        {#- "NOTE4": "NOTE: Partitions are for OSD nodes. All other partitions are coded into the given ks file.", #}
        "ports": {
          "http": 80,
          "https": 443,
          "xmlrpc": 25151
        },
        "repo_mirror": {%- if pxe_boot.repo_mirror == True %} true {% else %} false {%- endif %}
      },
      {% endif %}
      "os": {
        "type": "{{ os.type }}",
        "name": "{{ os.name }}",
        "version": "{{ os.version }}",
        "arch": "{{ os.arch }}",
        "distro": "{{ os.distro }}",
        "breed": "{{ os.breed }}"
        {%- if os.type == "rhel" %}
        ,"subscription": {
            "enable": {%- if subscription.enable == True %} true, {% else %} false, {%- endif %}
            "type": "{{ subscription.type }}",
            "server": "{{ subscription.server }}",
            "key": "{{ subscription.key }}"
          },
        {%- endif %}
      },

        {#- NOTE: WIP - TODO: Remove CEPH section HERE and move all non ceph-chef attributes to 'cepheus' section only! #}

      "ceph": {
        "cluster": "{{ ceph.cluster }}",
        "repo": {
          "create": {%- if ceph_repo_create == True %} true, {% else %} false, {%- endif %}
          "version": {
            "name": "{{ ceph.name }}",
            "number": "{{ ceph.version }}",
            "branch": "{{ ceph_repo.branch }}",
            "revision": "{{ ceph_repo.revision }}",
            "arch": "{{ ceph_repo.arch }}"
          }
        },
        "mgr": {
          "enable": {%- if ceph.mgr.enable == True %} true {% else %} false {%- endif %}
        },
        "config": {
          {#- "NOTE": "This section is pure key/value. Meaning, the key and value are added to the given location in ceph.conf.", #}
          "global": {
            "rgw override bucket index max shards": {{ ceph.radosgw.bucket_shards }}
          },
          "mon": {
            {#- 'mon allow pool delete' = false *AFTER* the cluster is stable. Keeps someone from deleting pools :) #}
            "mon osd full ratio": 0.95,
            "mon osd nearfull ratio": 0.85,
            "mon pg warn max per osd": 0,
            "mon pg warn max object skew": 10,
            "mon osd min down reporters": 3,
            "mon osd down out interval": 600,
            "clock drift allowed": 0.15
          },
          "radosgw": {
            "cache max file size": 20000000
          }
        },
        "mon": {
          "port": {{ ceph.mon.port }},
          "niceness": {{ ceph.mon.niceness }},

          {#- NB: Need to remove the bond portion of here and the recipe #}
          "bond": {
            "enable": {%- if bond.enable == True %} true, {% else %} false, {%- endif %}
            "name": "{{ bond.name }}",
            "type": "Bond",
            "mtu": {{ bond.mtu }},
            "interfaces": ["{{ bond.interfaces|join('","') }}"],
            "options": "{{ bond.options }}",
            "nm_controlled": "no"
          }

        },
        "radosgw": {
          "port": {{ ceph.radosgw.default_port }},
          "default_url": "{{ ceph.radosgw.default_url }}",
          "debug": {
            "logs": {
              "level": {{ ceph.radosgw.debug.logs.level }},
              "enable": {%- if ceph.radosgw.debug.logs.enable == True %} true {% else %} false {%- endif %}
            }
          },
          "logs": {
            "ops": {%- if ceph.radosgw.logs.ops.enable == True %} true, {% else %} false, {%- endif %}
            "usage": {%- if ceph.radosgw.logs.usage.enable == True %} true {% else %} false {%- endif %}
          },
          "rgw_webservice": {
            "enable": {%- if ceph.radosgw.rgw_webservice.enable == True %} true, {% else %} false, {%- endif %}
            "port": {{ ceph.radosgw.rgw_webservice.port }},
            "user": "{{ ceph.radosgw.rgw_webservice.user }}"
          },
          "gc": {
            "max_objects": {{ ceph.radosgw.gc.max_objects }},
            "obj_min_wait": {{ ceph.radosgw.gc.obj_min_wait }},
            "processor_max_time": {{ ceph.radosgw.gc.processor_max_time }},
            "processor_period": {{ ceph.radosgw.gc.processor_period }}
          },
          "users": [
            {%- for user in ceph.radosgw.users %}
              {"uid": "{{ user.uid }}", "name": "{{ user.name }}", "email": "{{ user.email }}", "admin_caps": "{{ user.admin_caps }}", "access_key": "{{ user.access_key }}", "secret_key": "{{ user.secret_key }}", "max_buckets": {{ user.max_buckets }}, "status": "{{ user.status }}", "key_type": "{{ user.key_type }}", "quota": {"user": { "status": "{{ user.quota.user.status }}", "size": {{ user.quota.user.size }}, "objects": {{ user.quota.user.objects }} }, "buckets": [{%- for bucket in user.quota.buckets %}{"name": "{{ bucket.name }}", "status": "{{ bucket.status }}", "size": {{ bucket.size }}, "objects": {{ bucket.objects }}}{%- if not loop.last %},{% endif %}{%- endfor %}]}, "zones": [{%- for zone in user.zone %}"{{ zone }}"{%- if not loop.last %},{% endif %}{% endfor %}], "buckets": [{%- for bucket in user.buckets %}{"name": "{{ bucket.name }}", "acl": "{{ bucket.acl }}"}{%- if not loop.last %},{% endif %}{% endfor %}]}{%- if not loop.last %},{% endif %}
            {%- endfor %}
          ],
          "rgw_num_rados_handles": {{ ceph.radosgw.rados_handles }},
          "civetweb_num_threads": {{ ceph.radosgw.civetweb_threads }},
          "bond": {
            "enable": {%- if bond.enable == True %} true, {% else %} false, {%- endif %}
            "name": "{{ bond.name }}",
            "type": "Bond",
            "mtu": {{ bond.mtu }},
            "interfaces": ["{{ bond.interfaces|join('","') }}"],
            "options": "{{ bond.options }}",
            "nm_controlled": "no"
          }
        },
        "osd": {
          {#- Move this data to the seed yamls if we need to make them more dynamic for different clusters. #}
          "devices": [
              {%- for item in ceph.osd.devices %}
                {"data": "{{ ceph.osd.device }}{{ item.device }}", "data_type": "{{ item.data_type }}", "journal": "{{ ceph.osd.device }}{{ item.journal }}", "journal_type": "{{ item.journal_type }}", "encrypted": {% if item.encrypted == True %}true{% else %}false{%- endif %}}{%- if not loop.last %},{% endif %}
              {%- endfor %}
          ],
          "crush": {
            "update": {%- if ceph.osd.crush.update_crush == True %} true, {% else %} false, {%- endif %}
            "update_on_start": {%- if ceph.osd.crush.update_on_start == True %} true, {% else %} false, {%- endif %}
            "chooseleaf_type": {{ ceph.osd.crush.chooseleaf_type }}
          },
          {#- The add and remove should reflect the same structure as devices above and should be used for maintenance only! #}
          "add": [],
          "remove": [],
          "niceness": {{ ceph.osd.niceness }},
          {#- "encrypted": false, #}
          "rebalance": {%- if ceph.osd.rebalance == True %} true, {% else %} false, {%- endif %}
          "size": {
            "max": 3,
            "min": 2
          },
          "journal": {
            {#- Change the journal size for the prod SSD #}
            {#- Calculate min size (if you have space then use it): 2 * (#OSDs in node * 100MB/s) * 5 (default sync interval) = 2 * (12 * 100) * 5 = 12GB #}
            "size": {{ ceph.osd.journal.size }}
          }
        },
        "pools": {
          "active": [
            "{{ ceph.pools.active|join('", "') }}"
          ],
          "version": {{ ceph.pools.version }},
          "crush": {
            "rule": {{ ceph.pools.crush_rule }}
          },
          "erasure_coding": {
            "profiles": [
              {"profile": "{{ ceph.ec.profile }}", "directory": "{{ ceph.ec.directory }}", "plugin": "{{ ceph.ec.plugin }}", "force": true, "technique": "{{ ceph.ec.technique }}", "ruleset_root": "{{ ceph.ec.ruleset_root }}", "ruleset_failure_domain": "{{ ceph.ec.ruleset_failure_domain }}", "key_value": {"k": {{ ceph.ec.k }}, "m": {{ ceph.ec.m }}}}
            ]
          },
          "radosgw": {
            "federated": {
              "enable": {%- if ceph.radosgw.federated.enable == True %} true, {% else %} false, {%- endif %}
              "multisite_replication": false,
              {#- Instance name MUST match VIPs name and Backend/Server instance variables. IF Federation is not used then Backend/Server instance variable should be empty. #}
              "zone_instances": [
                {%- for item in adc.vip.vips %}
                  {%- if item.type == "rgw" %}
                  {"name": "{{ item.name }}", "port": {{ item.backend_port }}, "region": "{{ item.region }}", "url": "{{ item.url }}", "rgw_thread_pool": {{ item.rgw_thread_pool }}, "handles": {{ item.handles }}, "threads": {{ item.threads }}}{%- if not loop.last %},{% endif %}
                  {%- endif %}
                {%- endfor %}
              ],
              "regions": ["{{ ceph.radosgw.federated.regions|join('", "') }}"],
              "enable_regions_zones": {%- if ceph.radosgw.federated.enable_regions_zones == True %} true, {% else %} false, {%- endif %}
              "master_zone": "{{ ceph.radosgw.federated.master_zone }}",
              "master_zone_port": {{ ceph.radosgw.federated.master_zone_port }}
            },
            {#- The data_percent below should equal 100%. If Federated then the calculation will take those pools into account. #}
            {#- NOTE: IMPORTANT - ONLY 'buckets' (data) can be erasure coded. Others need to be replicated. #}
            "pools": [
                {%- for item in ceph.pools.radosgw %}
                  {"name": "{{ item.name }}", "data_percent": {{ "{:,.2f}".format(item.data_percent) }}, "type": "{{ item.type }}", "profile": "{{ item.profile }}", "crush_ruleset": {{ item.crush_ruleset }}, "crush_ruleset_name": "{{ item.crush_ruleset_name }}"}{%- if not loop.last %},{% endif %}
                {%- endfor %}
            ],
            {#- If federated then the settings below apply to the federated pools else the non-federated pools. #}
            "settings": {
              "pg_num": {{ ceph.radosgw.settings.pg_num }},
              "pgp_num": {{ ceph.radosgw.settings.pgp_num }},
              "options": "{{ ceph.radosgw.settings.options }}",
              "force": {%- if ceph.radosgw.settings.force == True %} true, {% else %} false, {%- endif %}
              "calc": {%- if ceph.radosgw.settings.calc == True %} true, {% else %} false, {%- endif %}
              "size": {{ ceph.radosgw.settings.size }},
              "crush_ruleset": {{ ceph.radosgw.settings.crush_ruleset }},
              "chooseleaf": "{{ ceph.radosgw.settings.chooseleaf }}",
              "chooseleaf_type": {{ ceph.radosgw.settings.chooseleaf_type }},
              "type": "{{ ceph.radosgw.settings.type }}",
              "nodes_per_rack": {{ ceph.radosgw.settings.nodes_per_rack }}
            },
            "remove": {
              "names": []
            }
          },
          "pgs": {
            "calc": {
              "total_osds": {{ ceph.pgs.calc.total_osds }},
              "target_pgs_per_osd": {{ ceph.pgs.calc.target_pgs_per_osd }},
              "min_pgs_per_pool": {{ ceph.pgs.calc.min_pgs_per_pool }},
              "replicated_size": {{ ceph.pgs.calc.replicated_size }},
              "erasure_size": {{ ceph.pgs.calc.erasure_size }}
            },
            "num": {{ ceph.pgs.num }}
          }
        },
        "restapi": {
          "url": "{{ ceph.restapi.url }}",
          "ip": "{{ ceph.restapi.ip }}",
          "port": {{ ceph.restapi.port }}
        }
      },
      "domain_name" : "{{ domain }}",
      "network": {
        "public": {
          "interface": "{{ network.public.interface }}",
          "cidr": [
            "{{ network.public.cidr|join('", "') }}"
          ],
          "mtu": {{ network.public.mtu }}
        },
        "cluster": {
          "interface": "{{ network.cluster.interface }}",
          "gateway_enable": {%- if network.gateway_enable == True %} true, {% else %} false, {%- endif %}
          "cidr": [
            "{{ network.cluster.cidr|join('", "') }}"
          ],
          "route": {
            "cidr": "{{ network.cluster.route.cidr }}"
          },
          "mtu": {{ network.cluster.mtu }}
        }
      },
      "dns": {
        "servers": ["{{ nameservers|join('", "') }}"]
      },
        "ntp": {
            "servers": ["{{ ntp|join('", "') }}"]
        },
        "monitoring": {
            "enable": {%- if monitoring.enable == True %} true, {% else %} false, {%- endif %}
            "collectd-plugins": {
                "write_graphite": {
                    "node": {
                        "id": "{{ monitoring.collectd.id }}",
                        "host": "{{ monitoring.collectd.ip }}",
                        "port": {{ monitoring.collectd.port }},
                        "prefix": "{{ monitoring.collectd.prefix }}"
                    }
                }
            },
            "zabbix": {
                "repository": "http://repo.zabbix.com/zabbix/2.4/rhel/7/x86_64/",
                "repository_key": "file:///etc/pki/rpm-gpg/RPM-GPG-KEY-ZABBIX",
                "server": "{{ monitoring.zabbix.server }}",
                "ceph": {
                    "blocked_op": {{ monitoring.zabbix.ceph.blocked_op }},
                    "slow_osd": {{ monitoring.zabbix.ceph.slow_osd }}
                }
            }
        },
        "chef_client": {
            "server_url": "{{ chef.server }}",
            "cache_path": "/var/chef/cache",
            "backup_path": "/var/chef/backup",
            "validation_client_name": "{{ chef.validation_client_name }}",
            "run_path": "/var/chef"
        }
    }
  }
}
